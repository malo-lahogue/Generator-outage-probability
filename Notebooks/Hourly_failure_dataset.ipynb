{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c884896d",
   "metadata": {},
   "source": [
    "# Create the failure dataset at hourly resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db40ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "import data_processing as dp\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e08e3",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c4e319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Events : 2013\n",
      "Loading Events : 2014\n",
      "Loading Events : 2015\n",
      "Loading Events : 2016\n",
      "Loading Events : 2017\n",
      "Loading Events : 2018\n",
      "Loading Events : 2019\n",
      "Loading Events : 2020\n",
      "Loading Events : 2021\n",
      "Loading Events : 2022\n",
      "Loading Events : 2023\n",
      "Loading Events : 2024\n",
      "Warning: Ignoring columns ['RegionCode'] in GADS_Events_2024_20250505.xlsx\n",
      "Loaded 6005103 events\n"
     ]
    }
   ],
   "source": [
    "events_df = dp.load_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec82ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering events by cause codes...\n",
      "Kept 839971 events out of 6005103 (13.99%) after filtering by cause codes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging duplicated events: 100%|██████████| 839971/839971 [00:11<00:00, 73947.19it/s] \n",
      "Adding state and region: 100%|██████████| 834847/834847 [01:08<00:00, 12177.04it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(dp)\n",
    "\n",
    "filtered_events_df = dp.filter_events(events_df, \n",
    "                                      CauseCodes=['U1', 'U2', 'U3', 'D1', 'D2', 'D3', 'SF'], \n",
    "                                      filter_fuel=False, \n",
    "                                      exclude_states=['Other','Mexico','South America'], \n",
    "                                      include_states=None,\n",
    "                                      add_fuel_failure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9deb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Performances : 2013\n",
      "Loading Performances : 2014\n",
      "Loading Performances : 2015\n",
      "Loading Performances : 2016\n",
      "Loading Performances : 2017\n",
      "Loading Performances : 2018\n",
      "Loading Performances : 2019\n",
      "Loading Performances : 2020\n",
      "Loading Performances : 2021\n",
      "Loading Performances : 2022\n",
      "Warning: Ignoring columns ['PumpingHours'] in GADS_Performance_2022_20230605.xlsx\n",
      "Loading Performances : 2023\n",
      "Warning: Ignoring columns ['PumpingHours'] in GADS_Performance_2023_20240422.xlsx\n",
      "Loading Performances : 2024\n",
      "Warning: Ignoring columns ['RegionCode', 'PumpingHours'] in GADS_Performance_2024_20250505.xlsx\n",
      "Loaded 1035652 events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9305/9305 [00:07<00:00, 1173.11it/s]\n"
     ]
    }
   ],
   "source": [
    "units_start_end = dp.get_units_start_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741e1f9",
   "metadata": {},
   "source": [
    "## Format transition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3695c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [18:56<00:00,  6.30it/s]\n"
     ]
    }
   ],
   "source": [
    "transition_data_compressed = defaultdict(pd.DataFrame)\n",
    "\n",
    "for unit_id, events_unit in tqdm(filtered_events_df.groupby('UnitID')):\n",
    "    # Store the transitions for this unit\n",
    "    unit_transitions_data = {\"Datetime\":[], # at the start of each hour\n",
    "                            \"Initial_gen_state\":[],\n",
    "                            \"Final_gen_state\":[]\n",
    "                            }\n",
    "    # Get the start and end time for this unit\n",
    "    start_time_unit = units_start_end[units_start_end['UnitID'] == unit_id]['First_Operation_Date'].values[0]\n",
    "    end_time_unit = units_start_end[units_start_end['UnitID'] == unit_id]['Last_Operation_Date'].values[0]\n",
    "    current_time = start_time_unit\n",
    "\n",
    "    # Unit metadata\n",
    "    unit_geo_state = events_unit[\"State\"].iloc[0]\n",
    "    unit_technology = events_unit[\"UnitTypeCodeName\"].iloc[0]\n",
    "\n",
    "    # Sort events by start date\n",
    "    events_unit = events_unit.sort_values('EventStartDT')\n",
    "    last_state = 'A'\n",
    "\n",
    "    for event_idx, event in events_unit.iterrows():\n",
    "        event_start = event['EventStartDT'].replace(minute=0, second=0, microsecond=0)\n",
    "        event_end = event['EventEndDT'].replace(minute=0, second=0, microsecond=0) + pd.Timedelta(hours=1)\n",
    "        state_during_event = event[\"EventTypeCode\"]\n",
    "        if state_during_event.startswith('D'):\n",
    "            state_during_event = 'D'\n",
    "        elif state_during_event.startswith('U'):\n",
    "            state_during_event = 'U'\n",
    "        elif state_during_event == 'SF':\n",
    "            if last_state != 'A':\n",
    "                state_during_event = 'U' # Startup failure treated as outage if the unit was not available before\n",
    "            else:\n",
    "                continue # skip startup failure if the unit was available before\n",
    "        \n",
    "\n",
    "        # Record 'A' states until the event starts\n",
    "        while current_time < event_start:\n",
    "            unit_transitions_data[\"Datetime\"].append(current_time)\n",
    "            unit_transitions_data[\"Initial_gen_state\"].append(last_state)\n",
    "            unit_transitions_data[\"Final_gen_state\"].append('A')\n",
    "            current_time += pd.Timedelta(hours=1)\n",
    "            last_state = 'A'\n",
    "        \n",
    "        # Record states during the event\n",
    "        while current_time < event_end:\n",
    "            unit_transitions_data[\"Datetime\"].append(current_time)\n",
    "            unit_transitions_data[\"Initial_gen_state\"].append(last_state)\n",
    "            unit_transitions_data[\"Final_gen_state\"].append(state_during_event)\n",
    "            current_time += pd.Timedelta(hours=1)\n",
    "            last_state = state_during_event\n",
    "    \n",
    "    # Record 'A' states until the end of the unit's operation\n",
    "    while current_time <= end_time_unit:\n",
    "        unit_transitions_data[\"Datetime\"].append(current_time)\n",
    "        unit_transitions_data[\"Initial_gen_state\"].append(last_state)\n",
    "        unit_transitions_data[\"Final_gen_state\"].append('A')\n",
    "        current_time += pd.Timedelta(hours=1)\n",
    "        last_state = 'A'\n",
    "\n",
    "    unit_transitions_df = pd.DataFrame(unit_transitions_data)\n",
    "\n",
    "    transition_data_compressed[unit_id] = {\n",
    "                                            \"metadata\": {\n",
    "                                                \"UnitID\": unit_id,\n",
    "                                                \"State\": unit_geo_state,\n",
    "                                                \"Technology\": unit_technology\n",
    "                                            },\n",
    "                                            \"transitions\": unit_transitions_df\n",
    "                                          }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf9eac",
   "metadata": {},
   "source": [
    "## Export yearly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60fb223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2013...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 948.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2014...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 971.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2015...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 961.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2016...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 982.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2017...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 983.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2018...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 983.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2019...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 972.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 959.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2021...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 977.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2022...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 981.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2023...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 976.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting year 2024...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7166/7166 [00:07<00:00, 982.58it/s] \n"
     ]
    }
   ],
   "source": [
    "compressed = True\n",
    "\n",
    "for year in range(2013, 2024 + 1):\n",
    "    print(f\"Exporting year {year}...\")\n",
    "    yearly_data = []\n",
    "    for unit_id, unit_data in tqdm(transition_data_compressed.items()):\n",
    "        unit_transitions_df = unit_data[\"transitions\"]\n",
    "        # Filter for the current year\n",
    "        yearly_transitions_df = unit_transitions_df[unit_transitions_df[\"Datetime\"].dt.year == year].copy()\n",
    "        unit_geo_state = unit_data[\"metadata\"][\"State\"]\n",
    "        unit_technology = unit_data[\"metadata\"][\"Technology\"]\n",
    "\n",
    "        yearly_transitions_df[\"Geographical State\"] = unit_geo_state\n",
    "        yearly_transitions_df[\"Technology\"] = unit_technology\n",
    "\n",
    "        yearly_data.append(yearly_transitions_df)\n",
    "    yearly_data_df = pd.concat(yearly_data, ignore_index=True)\n",
    "    \n",
    "    if compressed :\n",
    "        compressed_df = yearly_data_df.groupby(['Datetime', 'Initial_gen_state', 'Final_gen_state', 'Geographical State', 'Technology']).size().reset_index(name='Count')\n",
    "        compressed_df.to_csv(f\"../Data/hourly_failure_dataset_compressed_{year}.csv\", index=False)\n",
    "    else:\n",
    "        yearly_data_df.to_csv(f\"../Data/hourly_failure_dataset_{year}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c30252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScenarioGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
